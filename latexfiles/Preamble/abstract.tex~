\section{Abstract}
Every internet user today is exposed to countless article headlines. These can range from informative, to sensationalist, to downright misleading. These snippets of information can have tremendous impacts on those exposed and can even shape ones views on a subject before even reading the associated article. For these reasons and more, it is important that the Natural Language Processing community turn its attention towards this critical part of everyday life by improving current abstractive text summarization techniques. To aid in that endeavor, this project explores various methods of teacher forcing, a technique used during model training for sequence-to-sequence recurrent reural network architectures.

A relatively new deep learning library called PyTorch has made experimentation with teacher forcing accessible for the first time and is utilized for this purpose in the project. Additionally, to the author's best knowledge this is the first implementation of abstractive headline summarization in PyTorch. Seven different teacher forcing techniques were designed and experimented with: (1) Constant levels of  0\%, 25\%, 50\%, 75\%, and 100\% teacher forcing through the entire training cycle, and (2) two different graduated techniques: one that decreased linearly from 100\% to 0\% through the entire training cycle to convergence, and another that reset every 12.5\% through the training cycle, often corresponding with learning rate annealing.

These seven different teacher forcing techniques are compared to one another via two metrics: (1) ROUGE F-scores, the most common metric used in this field as well as training loss over time, and (2) average loss over time. Counter to what was expected, this project shows with statistical significance that consistent 100\% and 75\% teacher forcing produced better ROUGE scores than any other metric, a technique that aligns with existing models. These two methods also convered the quickest and had the lowest loss levels.