\subsubsection{History}
Neural Networks (NN) were first theorised dating back to 1943 but what we think of today as an ANN can best be recognized in Rosenblatt's seminal perceptron publication \cite{Rosenblatt1958}. However, lack of elements found in modern neural networks such as backpropogation (\ref{backprop}) in the learning algorithm emplyed by Rosenblatt failed to adequately ``endow the perceptronâ€™s predictions with meaningful probabilistic interpretations, or derive the perceptron as a maximum likelihood estimation algorithm'' \cite{Ng2011a}. Regardless, research continued until Minsky and Papert's book \textit{Perceptron} in 1969 which detailed the limitations of the perceptron as a simple linear classifier incapable of complex operations such as XOR \cite{Minsky1969a}. This stalled progress on ANN development considerably until the 1980s when Backpropogation was applied to Neural Networks by Werbos. \cite{Schmidhuber2015}