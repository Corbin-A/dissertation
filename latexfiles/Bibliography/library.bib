Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Davidson2003,
abstract = {Bootstrap methods are resampling techniques for assessing uncertainity. They are useful when inference is to be based on a complex procedure for which theoretical results are unavailable or not useful for the sample sizes met in practice, where a standard model is suspect but it is unclear with what to replace it, or where a 'quick and dirty' answer is required. They can also be used to verify the usefulness of standard approximations for parametric methods, and to improve them if they seem to give inadequate references. This article, a brief introduction on their use, is based soley on parts of Davison and Hinkley (1997), where further details and many examples and practicals can be found. A different point of view is given by Efron and Tibshirani (1993) and a more mathematical survey by Shao and Tu (1995), while Hall (1992) describes the underlying theory.},
author = {Davidson, A. C. and Kuonen, D.},
journal = {Statistical Computing {\&} Statistical Graphics Newsletter},
keywords = {R,bootstrap},
number = {1},
pages = {6--11},
title = {{An introduction to the bootstrap with application in R}},
url = {http://www.statoo.com/en/publications/bootstrap{\_}scgn{\_}v131.pdf},
volume = {13},
year = {2003}
}
@misc{Robertson2017,
author = {Robertson, Sean},
title = {{Translation with a Sequence to Sequence Network and Attention}},
url = {http://pytorch.org/tutorials/intermediate/seq2seq{\_}translation{\_}tutorial.html},
urldate = {2017-08-13},
year = {2017}
}
@article{Gulrajani2017,
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes significant progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these training failures are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative method for enforcing the Lipschitz constraint: instead of clipping weights, penalize the norm of the gradient of the critic with respect to its input. Our proposed method converges faster and generates higher-quality samples than WGAN with weight clipping. Finally, our method enables very stable GAN training: for the first time, we can train a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.},
archivePrefix = {arXiv},
arxivId = {1704.00028},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
doi = {10.1016/j.aqpro.2013.07.003},
eprint = {1704.00028},
isbn = {0030-8870},
issn = {00308870},
journal = {arXiv Preprints},
pages = {1--19},
pmid = {24439530},
title = {{Improved Training of Wasserstein GANs}},
url = {http://arxiv.org/abs/1704.00028},
year = {2017}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Narayan2017,
abstract = {Most extractive summarization methods focus on the main body of the document from which sentences need to be extracted. The gist of the document often lies in the side information of the document, such as title and image captions. These types of side information are often available for newswire articles. We propose to explore side information in the context of single-document extractive summarization. We develop a framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor with attention over side information. We evaluate our models on a large scale news dataset. We show that extractive summarization with side information consistently outperforms its counterpart (that does not use any side information), in terms on both informativeness and fluency.},
archivePrefix = {arXiv},
arxivId = {1704.04530},
author = {Narayan, Shashi and Papasarantopoulos, Nikos and Lapata, Mirella and Cohen, Shay B.},
eprint = {1704.04530},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Narayan et al. - 2017 - Neural Extractive Summarization with Side Information.pdf:pdf},
title = {{Neural Extractive Summarization with Side Information}},
url = {http://arxiv.org/abs/1704.04530},
year = {2017}
}
@article{Kruskal1952,
abstract = {Abstract Given C samples, with n i observations in the ith sample, a test of the hypothesis that the samples are from the same population may be made by ranking the observations from from 1 to $\Sigma$n i (giving each observation in a group of ties the mean of the ranks tied for), finding the C sums of ranks, and computing a statistic H. Under the stated hypothesis, H is distributed approximately as $\chi$2(C – 1), unless the samples are too small, in which case special approximations or exact tables are provided. One of the most important applications of the test is in detecting differences among the population means.* * Based in part on research supported by the Office of Naval Research at the Statistical Research Center, University of Chicago.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Kruskal, William H. and Wallis, W. Allen},
doi = {10.1080/01621459.1952.10483441},
eprint = {NIHMS150003},
isbn = {01621459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {260},
pages = {583--621},
pmid = {21900426},
title = {{Use of Ranks in One-Criterion Variance Analysis}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
volume = {47},
year = {1952}
}
@article{Luhn1958,
abstract = {Excerpts of technical papers and magazine articles that serve the purposes of conventional abstracts have been created entirely by automatic means. In the exploratory research described, the complete text of an article in machine-readable form is scanned by an IBM 704 data-processing machine and analyzed in accordance with a standard program. Statistical information derived from word frequency and distribution is used by the machine to compute a relative measure of significance, first for individual words and then for sentences. Sentences scoring highest in significance are extracted and printed out to become the “auto-abstract."},
author = {Luhn, H. P.},
doi = {10.1147/rd.22.0159},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
number = {2},
pages = {159--165},
title = {{The Automatic Creation of Literature Abstracts}},
volume = {2},
year = {1958}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{Werbos1988,
abstract = {Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place. ?? 1988.},
author = {Werbos, Paul J.},
doi = {10.1016/0893-6080(88)90007-X},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Werbos - 1988 - Generalization of backpropagation with application to a recurrent gas market model.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Backpropagation,Cerebral cortex,Continuous time,Energy models,Modelling,Prediction,Recurrent,Reinforcement learning},
number = {4},
pages = {339--356},
title = {{Generalization of backpropagation with application to a recurrent gas market model}},
volume = {1},
year = {1988}
}
@article{Williams1989,
abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Williams, Ronald J. and Zipser, David},
doi = {10.1162/neco.1989.1.2.270},
eprint = {arXiv:1011.1669v3},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {270--280},
pmid = {20505160},
title = {{A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.2.270},
volume = {1},
year = {1989}
}
@misc{Ford2015,
author = {Ford, Clay},
title = {{Understanding Q-Q Plots}},
url = {http://data.library.virginia.edu/understanding-q-q-plots/},
urldate = {09/13/2017},
year = {2015}
}
@misc{Greff2016,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, Klaus and Srivastava, Rupesh K. and Koutnik, Jan and Steunebrink, Bas R. and Schmidhuber, Jurgen},
booktitle = {IEEE Transactions on Neural Networks and Learning Systems},
doi = {10.1109/TNNLS.2016.2582924},
eprint = {1503.04069},
isbn = {9788578110796},
issn = {21622388},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
year = {2016}
}
@misc{Boyle1997,
author = {Boyle, Joe},
title = {{Choosing an Appropriate Measure of Central Tendency}},
url = {https://simon.cs.vt.edu/SoSci/converted/MMM/choosingct.html},
urldate = {2017},
year = {1997}
}
@misc{Chintala2017,
author = {Chintala, Soumith},
keywords = {Chintala2017},
title = {{Autograd: automatic differentiation}},
url = {http://pytorch.org/tutorials/beginner/blitz/autograd{\_}tutorial.html},
urldate = {2017-10-07},
year = {2017}
}
@inproceedings{Dosovitskiy2015,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Brox, Thomas},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1512.03385},
isbn = {9781467369640},
issn = {10636919},
pages = {1538--1546},
pmid = {18267787},
title = {{Learning to generate chairs with convolutional neural networks}},
volume = {07-12-June},
year = {2015}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart, Hinton, Williams - 1986 - Learning representations by back-propagating errors.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learning+representations+by+back-propagating+errors{\&}ots=zZDj2mGYVQ{\&}sig=mcyEACaE{\_}ZB4FB4xsoTgXgcbE2g{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Lea},
volume = {323},
year = {1986}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
isbn = {9781937284961},
issn = {09205691},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1017/CBO9781139058452},
eprint = {arXiv:1406.2661v1},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
pmid = {1000183096},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{Ecker2014,
abstract = {Information presented in news articles can be misleading without being blatantly false. Experiment 1 examined the effects of misleading headlines that emphasize secondary content rather than the article's primary gist. We investigated how headlines affect readers' processing of factual news articles and opinion pieces, using both direct memory measures and more indirect reasoning measures. Experiment 2 examined an even more subtle type of misdirection. We presented articles featuring a facial image of one of the protagonists, and examined whether the headline and opening paragraph of an article affected the impressions formed of that face even when the person referred to in the headline was not the person portrayed. We demonstrate that misleading headlines affect readers' memory, their inferential reasoning and behavioral intentions, as well as the impressions people form of faces. On a theoretical level, we argue that these effects arise not only because headlines constrain further information processing, biasing readers toward a specific interpretation, but also because readers struggle to update their memory in order to correct initial misconceptions. Practical implications for news consumers and media literacy are discussed. (PsycINFO Database Record (c) 2014 APA, all rights reserved).},
author = {Ecker, Ullrich K H and Lewandowsky, Stephan and Chang, Ee Pin and Pillai, Rekha},
doi = {10.1037/xap0000028},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Ecker et al. - 2014 - The effects of subtle misinformation in news headlines.pdf:pdf},
isbn = {1939-2192},
issn = {1939-2192},
journal = {Journal of experimental psychology. Applied},
keywords = {inference; memory; news media; reading comprehensi},
number = {4},
pages = {323--35},
pmid = {25347407},
title = {{The effects of subtle misinformation in news headlines.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25347407},
volume = {20},
year = {2014}
}
@article{Rony2017,
abstract = {The use of alluring headlines (clickbait) to tempt the readers has become a growing practice nowadays. For the sake of existence in the highly competitive media industry, most of the on-line media including the mainstream ones, have started following this practice. Although the wide-spread practice of clickbait makes the reader's reliability on media vulnerable, a large scale analysis to reveal this fact is still absent. In this paper, we analyze 1.67 million Facebook posts created by 153 media organizations to understand the extent of clickbait practice, its impact and user engagement by using our own developed clickbait detection model. The model uses distributed sub-word embeddings learned from a large corpus. The accuracy of the model is 98.3{\%}. Powered with this model, we further study the distribution of topics in clickbait and non-clickbait contents.},
archivePrefix = {arXiv},
arxivId = {1703.09400},
author = {Rony, Md Main Uddin and Hassan, Naeemul and Yousuf, Mohammad},
eprint = {1703.09400},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Rony, Hassan, Yousuf - 2017 - Diving Deep into Clickbaits Who Use Them to What Extents in Which Topics with What Effects.pdf:pdf},
title = {{Diving Deep into Clickbaits: Who Use Them to What Extents in Which Topics with What Effects?}},
url = {http://arxiv.org/abs/1703.09400},
year = {2017}
}
@misc{Stanford2016,
abstract = {Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.},
author = {Stanford},
booktitle = {stanford.edu},
title = {{CS231N 2.1 Convolutional Neural Networks: Architectures, Convolution / Pooling Layers}},
url = {http://cs231n.github.io/},
year = {2016}
}
@phdthesis{Hochreiter1991,
abstract = {Ich versichere, da{\ss} ich diese Diplomarbeit selbst{\"{a}}ndig verfa{\ss}t und keine anderen als die ange-geben Quellen und Hilfsmittel benutzt habe.},
author = {Hochreiter, Josef},
booktitle = {Master's thesis, Institut fur Informatik, Technische Universitat, Munchen},
pages = {1--71},
school = {Technische Universitat, Munchen},
title = {{Untersuchungen zu dynamischen neuronalen Netzen}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Untersuchungen+zu+dynamischen+neuronalen+Netzen{\#}0},
year = {1991}
}
@article{Nielsen2017,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael A},
journal = {URL: http://neuralnetworksanddeeplearning.com/.(visited: 01.11.2016)},
title = {{Neural networks and deep learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2017}
}
@article{Ifantidou2009,
abstract = {This paper addresses the issue of newspaper-headline interpretation by questioning standard assumptions on how headlines are designed on the basis of largely prescriptive pragmatic guidelines or norms. The main questions examined are firstly, whether 'appropriate headlines' from the writer's perspective converge with 'effective headlines' from the reader's perspective, and secondly, whether there is a pragmatic heuristic which can explain in psychologically plausible terms the way headlines are selected and interpreted by newspaper readers. Drawing on 137 readers' reaction to a selection of UK/US newspaper headlines and on a corpus of 1310 reader-selected headlines, it is shown that headline readers tend to disregard standard norms such as length, clarity, and information as long as headlines rivet their attention in terms of creative style regardless of underdetermined semantic meaning. Using the framework of Relevance Theory (Sperber and Wilson, 1986/95; Wilson and Sperber, 2004; Wilson and Carston, 2007; Sperber and Wilson, 2008), it is suggested that readers select headlines guided by expectations of relevance and interpret headlines by creating occasion-specific ad hoc concepts and ad hoc contexts in an overall attempt to optimally ration processing effort with cognitive effects. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Ifantidou, Elly},
doi = {10.1016/j.pragma.2008.10.016},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Ifantidou - 2009 - Newspaper headlines and relevance Ad hoc concepts in ad hoc contexts.pdf:pdf},
isbn = {0378-2166},
issn = {03782166},
journal = {Journal of Pragmatics},
keywords = {Concept broadening/narrowing,Lexical adjustment,Radical inference,Underspecified context,±Creative,±Informative},
number = {4},
pages = {699--720},
title = {{Newspaper headlines and relevance: Ad hoc concepts in ad hoc contexts}},
volume = {41},
year = {2009}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Learning,Memory,Models,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neurological,Psychological,Short-Term,Time Factors},
number = {8},
pages = {1735--80},
pmid = {9377276},
title = {{Long Short-Term Memory}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9377276},
volume = {9},
year = {1997}
}
@article{Das2007,
abstract = {The increasing availability of online information has necessitated intensive research in the area of automatic text summarization within the Natural Lan- guage Processing (NLP) community. Over the past half a century, the prob- lem has been addressed from many different perspectives, in varying domains and using various paradigms. This survey intends to investigate some of the most relevant approaches both in the areas of single-document and multiple- document summarization, giving special emphasis to empirical methods and extractive techniques. Some promising approaches that concentrate on specific details of the summarization problem are also discussed. Special attention is devoted to automatic evaluation of summarization systems, as future research on summarization is strongly dependent on progress in this area.},
archivePrefix = {arXiv},
arxivId = {7182216},
author = {Das, Dipanjan and Martins, Andre F T},
doi = {10.1016/B0-08-044854-2/00957-3},
eprint = {7182216},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Das, Martins - 2007 - A Survey on Automatic Text Summarization.pdf:pdf},
isbn = {9780080448541},
issn = {0080448542},
journal = {Eighth ACIS International Conference on Software Engineering Artificial Intelligence Networking and ParallelDistributed Computing SNPD 2007},
pages = {574--578},
title = {{A Survey on Automatic Text Summarization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4287749},
volume = {4},
year = {2007}
}
@misc{Goodfellow2016,
abstract = {The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free.},
author = {{Goodfellow, Ian, Bengio, Yoshua, Courville}, Aaron},
booktitle = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org/},
year = {2016}
}
@article{Mao2016,
abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson {\$}\backslashchi{\^{}}2{\$} divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
archivePrefix = {arXiv},
arxivId = {1611.04076},
author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
eprint = {1611.04076},
journal = {arxiv},
pages = {1--16},
title = {{Least Squares Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.04076},
year = {2016}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Wilcoxon1945,
abstract = {The objective of the present paper is to indicate the possibility of using ranking methods.},
author = {Wilcoxon, Frank},
doi = {10.2307/3001968},
isbn = {0099-4987},
issn = {00994987},
journal = {Biometrics Bulletin},
number = {6},
pages = {80},
pmid = {20983181},
title = {{Individual Comparisons by Ranking Methods}},
url = {http://www.jstor.org/stable/10.2307/3001968?origin=crossref},
volume = {1},
year = {1945}
}
@article{Radev2002,
abstract = {To provide high-quality and safe care, clinicians must be able to optimally collect, distill, and interpret patient information. Despite advances in text summarization, only limited research exists on clinical summarization, the complex and heterogeneous process of gathering, organizing and presenting patient data in various forms.},
author = {Radev, Dragomir and Hovy, Eduard and McKeown, Kathleen},
doi = {10.1016/j.jbi.2011.03.008},
file = {:Users/c.albert/Research Papers/Dissertation/TextSummarization/preface.pdf:pdf},
isbn = {0891-2017},
issn = {1532-0480},
journal = {Computational linguistics},
number = {4},
pages = {399--408},
pmid = {21440086},
title = {{Introduction to the special issue on summarization}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21920798{\%}5Cnhttp://www.mitpressjournals.org/doi/abs/10.1162/089120102762671927},
volume = {28},
year = {2002}
}
@article{Bengio2015,
abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
archivePrefix = {arXiv},
arxivId = {1506.03099},
author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
doi = {10.1201/9781420049176},
eprint = {1506.03099},
file = {:Users/c.albert/Research Papers/Dissertation/1506.03099.pdf:pdf},
isbn = {0849371813},
issn = {1941-6016},
journal = {NIPS},
pages = {1--9},
pmid = {456984},
title = {{Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1506.03099},
year = {2015}
}
@article{Wenzlaff1981,
abstract = {Conducted 3 experiments to examine the effects that incriminating innuendo delivered by media sources have on audience impressions of innuendo targets. A total of 182 undergraduates served as Ss. The 1st study demonstrated innuendo effects by showing that audience impressions of a target were swayed in a negative direction by exposure to a prototypical innuendo headline, the incriminating question. A similar but substantially weaker effect was observed for an incriminating denial. The 2nd study showed that although variations in source credibility affected the persuasiveness of direct incriminating assertions, they had appreciably less impact on the persuasiveness of innuendos. In the 3rd study, the inferences an audience makes about the motives and knowledge of an innuendo source were investigated for their possible mediation of the innuendo effect. Audience inferences about the sensationalistic or muckraking qualities of the source were found to have a negligible influence on acceptance of innuendo from the source. The analysis also revealed that audiences commonly infer that the source is attempting to avoid charges of libel, which can reduce receptiveness to innuendo communication. (14 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Wenzlaff, Richard and Kerker, R Michael and Beattie, Ann E},
doi = {10.1037/0022-3514.40.5.822},
file = {:Users/c.albert/Research Papers/Dissertation/Misleading Title/MisleadingIfDontRead.pdf:pdf},
isbn = {0022-3514 1939-1315},
issn = {0022-3514},
journal = {Journal of Personality and Social Psychology},
number = {5},
pages = {822--832},
title = {{Incrimination through innuendo: Can media questions become public answers?}},
url = {http://psycnet.apa.org/journals/psp/40/5/822.html{\%}5Cnpapers3://publication/doi/10.1037/0022-3514.40.5.822},
volume = {40},
year = {1981}
}
@article{Nallapati2016a,
abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
archivePrefix = {arXiv},
arxivId = {1602.06023},
author = {Nallapati, Ramesh and Zhou, Bowen and dos Santos, Cicero Nogueira and Gulcehre, Caglar and Xiang, Bing},
eprint = {1602.06023},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Nallapati et al. - 2016 - Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond.pdf:pdf},
journal = {Proceedings of CoNLL},
month = {feb},
pages = {280--290},
title = {{Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond}},
url = {http://arxiv.org/abs/1602.06023},
year = {2016}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--15},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@misc{Olah2015,
abstract = {Humans don't start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don't throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can't do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It's unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. Recurrent Neural Networks have loops. In the above diagram, a chunk of neural network, , looks at some input and outputs a value . A loop allows information to be passed from one step of the network to the next. These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren't all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop: An unrolled recurrent neural network. This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They're the natural architecture of neural network to use for such data. And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning{\ldots} The list goes on. I'll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy's excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Olah, Christopher},
booktitle = {GITHUB colah blog},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
isbn = {9780874216561},
issn = {10282092},
keywords = {Blechnaceae,Indonesia,New species,Stenochlaena},
number = {2},
pages = {137--141},
pmid = {15003161},
title = {{Understanding LSTM Networks}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2017-08-11},
volume = {22},
year = {2015}
}
@article{Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/c.albert/Research Papers/Dissertation/BahdanauAttention.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
journal = {Iclr 2015},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2014}
}
@inproceedings{Venugopalan2016,
abstract = {Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).},
archivePrefix = {arXiv},
arxivId = {1505.00487},
author = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.515},
eprint = {1505.00487},
isbn = {9781467383912},
issn = {15505499},
pages = {4534--4542},
title = {{Sequence to sequence - Video to text}},
volume = {11-18-Dece},
year = {2016}
}
@misc{Liu2016,
author = {Liu, Peter and Pan, Xin},
booktitle = {Google Research Blog},
title = {{Text summarization with TensorFlow}},
url = {https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html},
urldate = {2017-04-20},
year = {2016}
}
@incollection{Nenkova2013,
abstract = {Numerous approaches for identifying important content for automatic text summarization have been developed to date. Topic representation approaches first derive an intermediate representation of the text that captures the topics discussed in the input. Based on these representa- tions of topics, sentences in the input document are scored for impor- tance. In contrast, in indicator representation approaches, the text is represented by a diverse set of possible indicators of importance which do not aim at discovering topicality. These indicators are combined, very often using machine learning techniques, to score the importance of each sentence. Finally, a summary is produced by selecting sentences in a greedy approach, choosing the sentences that will go in the summary one by one, or globally optimizing the selection, choosing the best set of sentences to form a summary. In this chapter we give a broad overview of existing approaches based on these distinctions, with particular at- tention on how representation, sentence scoring or summary selection strategies alter the overall performance of the summarizer. We also point out some of the peculiarities of the task of summarization which have posed challenges to machine learning approaches for the problem, and some of the suggested solutions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Nenkova, Ani and McKeown, Kathleen},
booktitle = {Mining Text Data},
chapter = {3},
doi = {10.1007/978-1-4614-3223-4},
eprint = {arXiv:1011.1669v3},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Nenkova, McKeown - 2013 - A Survey of Text Summarization Techniques.pdf:pdf},
isbn = {9781461432234},
issn = {1098-6596},
pages = {43--76},
pmid = {25246403},
title = {{A Survey of Text Summarization Techniques}},
year = {2013}
}
@article{See2017,
abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
archivePrefix = {arXiv},
arxivId = {1704.04368},
author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
doi = {10.18653/v1/P17-1099},
eprint = {1704.04368},
file = {:Users/c.albert/Research Papers/Dissertation/1704.04368.pdf:pdf},
journal = {ACL 2017},
title = {{Get To The Point: Summarization with Pointer-Generator Networks}},
url = {http://arxiv.org/abs/1704.04368},
year = {2017}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:Users/c.albert/Research Papers/Dissertation/reluICML.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@misc{Jolicoeur-Martineau2017,
author = {Jolicoeur-Martineau, Alexia},
title = {{Meow Generator}},
url = {https://ajolicoeur.wordpress.com/cats/},
urldate = {2017-07-03},
year = {2017}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Y and Simard, P and Frasconi, P},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
isbn = {1045-9227 VO - 5},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Schuster1997,
abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schuster, M. and Paliwal, K. K},
doi = {10.1109/78.650093},
eprint = {arXiv:1011.1669v3},
isbn = {1053-587X},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
number = {11},
pages = {2673--2681},
pmid = {25246403},
title = {{Bidirectional recurrent neural networks}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=650093},
volume = {45},
year = {1997}
}
@inproceedings{Bahdanau2016,
abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
archivePrefix = {arXiv},
arxivId = {1508.04395},
author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1508.04395},
isbn = {9780874216561},
issn = {13514180},
pages = {4945--4949},
pmid = {15991970},
title = {{End-to-End Attention-based Large Vocabulary Speech Recognition}},
url = {http://arxiv.org/abs/1508.04395},
year = {2016}
}
@article{AndrewGibiansky2015,
abstract = {Next, let's figure out how to do the exact same thing for convolutional neural networks. While the mathematical theory should be exactly the same, the actual derivation will be slightly more complex due to the architecture of convolutional neural networks.},
author = {{Andrew Gibiansky}},
journal = {CS231n Convolutional Neural Networks for Visual Recognition},
title = {{lecture 09 Convolutional Neural Network: Architectures, Convolution/Polling Layers}},
url = {http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/},
year = {2015}
}
@article{Luong2015,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
doi = {10.18653/v1/D15-1166},
eprint = {1508.04025},
file = {:Users/c.albert/Research Papers/Dissertation/LuongAttention.pdf:pdf},
isbn = {9781941643327},
issn = {10495258},
journal = {Emnlp},
keywords = {()},
number = {September},
pages = {11},
pmid = {14527267},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {http://arxiv.org/abs/1508.04025},
year = {2015}
}
@misc{Howard2017,
author = {Howard, Jeremy},
title = {{Introducing Pytorch for fast.ai}},
url = {http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/},
urldate = {2017-08-09},
year = {2017}
}
@article{Karpathy2015,
abstract = {There's something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I've in fact reached the opposite conclusion). Fast forward about a year: I'm training RNNs all the time and I've witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Karpathy, Andrej},
doi = {10.1017/CBO9781107415324.004},
eprint = {1512.03385},
isbn = {9781782166085},
issn = {1550-5499},
journal = {Web Page},
pages = {1--28},
pmid = {18267787},
title = {{The Unreasonable Effectiveness of Recurrent Neural Networks}},
url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
year = {2015}
}
@article{Mikolov2011,
abstract = {Better results by using Backpropagation throught time and better speed by using classes.},
author = {Mikolov, Tomas and Kombrink, S},
doi = {10.1109/ICASSP.2011.5947611},
isbn = {9781457705397},
issn = {1520-6149},
journal = {Icassp},
pages = {5528--5531},
title = {{Extensions of recurrent neural network language model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5947611},
year = {2011}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@misc{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the US Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
booktitle = {Neural Computation},
doi = {10.1162/neco.1989.1.4.541},
isbn = {0899-7667},
issn = {0899-7667},
number = {4},
pages = {541--551},
pmid = {1000111957},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.541},
volume = {1},
year = {1989}
}
@article{Jozefowicz2015,
abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1512.03385},
isbn = {9781937284978},
issn = {1045-9227},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {2342--2350},
pmid = {18267787},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
volume = {37},
year = {2015}
}
@article{Horne2016,
archivePrefix = {arXiv},
arxivId = {1703.09398},
author = {Horne, Benjamin D and Adalı, Sibel},
eprint = {1703.09398},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Horne, Adalı - 2016 - This Just In Fake News Packs a Lot in Title , Uses Simpler , Repetitive Content in Text Body , More Similar to S.pdf:pdf},
title = {{This Just In : Fake News Packs a Lot in Title , Uses Simpler , Repetitive Content in Text Body , More Similar to Satire than Real News}},
year = {2016}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v1},
author = {Schmidhuber, J},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v1},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in Neural Networks: An Overview}},
volume = {61},
year = {2015}
}
@article{Dor2003,
abstract = {This paper suggests an explanatory functional characterization of newspaper headlines. Couched within Sperber and Wilson's (1986) relevance theory, the paper makes the claim that headlines are designed to optimize the relevance of their stories for their readers: Headlines provide the readers with the optimal ratio between contextual effect and processing effort, and direct readers to construct the optimal context for interpretation. The paper presents the results of an empirical study conducted in the news-desk of one daily newspaper. It shows that the set of intuitive professional imperatives, shared by news-editors and copy-editors, which dictates the choice of headlines for specific stories, can naturally be reduced to the notion of relevance optimization. The analysis explains why the construction of a successful headline requires an understanding of the readers - their state-of-knowledge, their beliefs and expectations and their cognitive styles - no less than it requires an understanding of the story. It also explains the fact that skilled newspaper readers spend most of their reading time scanning the headlines - rather than reading the stories. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Dor, Daniel},
doi = {10.1016/S0378-2166(02)00134-0},
file = {:Users/c.albert/Research Papers/Dissertation/Misleading Title/On newspaper headlines as relevance optimizers{\_}Dorr.pdf:pdf},
isbn = {03782166},
issn = {03782166},
journal = {Journal of Pragmatics},
keywords = {Headlines,Media,News framing,News value,Pragmatics,Relevance theory,communication},
number = {5},
pages = {695--721},
title = {{On newspaper headlines as relevance optimizers}},
volume = {35},
year = {2003}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
journal = {arXiv:1701.07875},
pages = {30},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{Bhatia2015,
abstract = {Text Summarization was proved to be an advantage over manually summarizing the large data. It condenses the salient features from the text by preserving the content and serves the meaningful summary. Classification can be done in two ways – extractive and abstractive summarization. Extractive summarization uses statistical and linguistic features to determine the important features and fuse them into a shorter version. Whereas abstractive summarization understands the whole document and then generates the summary. In this paper extractive and abstractive methods are framed.},
author = {Bhatia, Neelima and Jaiswal, Arunima},
doi = {10.5120/20559-2947},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Bhatia, Jaiswal - 2015 - Trends in Extractive and Abstractive Techniques in Text Summarization.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
keywords = {Abstractive summarization,Extractive summarization},
number = {6},
pages = {975--8887},
title = {{Trends in Extractive and Abstractive Techniques in Text Summarization}},
volume = {117},
year = {2015}
}
@article{Sutskever2014,
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, O and Le, Quoc V.},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Nips},
pages = {1--9},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Mikolov2000,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
isbn = {2150-8097},
issn = {0003-6951},
journal = {CrossRef Listing of Deleted DOIs},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations ofWords and Phrases and their Compositionality}},
url = {http://www.crossref.org/deleted{\_}DOI.html},
volume = {1},
year = {2000}
}
@article{Nakamoto2008,
abstract = {Abstract. A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
archivePrefix = {arXiv},
arxivId = {43543534534v343453},
author = {Nakamoto, Satoshi},
doi = {10.1007/s10838-008-9062-0},
eprint = {43543534534v343453},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Nakamoto - 2008 - Bitcoin A Peer-to-Peer Electronic Cash System.pdf:pdf},
isbn = {978-972-757-716-3},
issn = {09254560},
journal = {www.Bitcoin.Org},
pages = {9},
pmid = {14533183},
title = {{Bitcoin: A Peer-to-Peer Electronic Cash System}},
url = {https://bitcoin.org/bitcoin.pdf},
year = {2008}
}
@article{Sennrich2016,
abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, so the translation of rare and unknown words is an open problem. Previous work addresses this problem through back-off dictionaries. In this paper, we introduce a simpler and more effective approach, enabling the translation of rare and unknown words by encoding them as sequences of subword units, based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 0.8 and 1.5 BLEU, respectively.},
archivePrefix = {arXiv},
arxivId = {1508.07909},
author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
eprint = {1508.07909},
file = {:Users/c.albert/Research Papers/Dissertation/Neural Machine Translation of Rare Words with Subword Units.pdf:pdf},
isbn = {9781510827585},
journal = {Acl 2016},
pages = {1715--1725},
title = {{Neural Machine Translation of Rare Words with Subword Units}},
url = {http://arxiv.org/abs/1508.07909},
year = {2016}
}
@inproceedings{Chopra2016,
abstract = {ive Sentence Summarization gener-ates a shorter version of a given sentence while attempting to preserve its meaning. We intro-duce a conditional recurrent neural network (RNN) which generates a summary of an in-put sentence. The conditioning is provided by a novel convolutional attention-based encoder which ensures that the decoder focuses on the appropriate input words at each step of genera-tion. Our model relies only on learned features and is easy to train in an end-to-end fashion on large data sets. Our experiments show that the model significantly outperforms the recently proposed state-of-the-art method on the Giga-word corpus while performing competitively on the DUC-2004 shared task.},
author = {Chopra, Sumit and Auli, Michael and Rush, Alexander M.},
booktitle = {NAACL-2016},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Chopra, Auli, Rush - 2016 - Abstractive Sentence Summarization with Attentive Recurrent Neural Networks.pdf:pdf},
isbn = {9781941643914},
pages = {93--98},
title = {{Abstractive Sentence Summarization with Attentive Recurrent Neural Networks}},
year = {2016}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144v2},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Learning},
pages = {1--23},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Hu2015,
abstract = {Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public$\backslash$footnote{\{}http://icrc.hitsz.edu.cn/Article/show/139.html{\}}. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.},
archivePrefix = {arXiv},
arxivId = {1506.05865},
author = {Hu, Baotian and Chen, Qingcai and Zhu, Fangze},
eprint = {1506.05865},
isbn = {9781941643327},
journal = {In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {September},
pages = {1967--1972},
title = {{LCSTS: A Large Scale Chinese Short Text Summarization Dataset}},
url = {http://arxiv.org/abs/1506.05865},
year = {2015}
}
@article{Elman1990,
abstract = {[PDF]},
author = {Elman, J L},
doi = {10.1207/s15516709cog1402_1},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Elman - 1990 - Finding structure in time 1.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
number = {2},
pages = {179--211},
pmid = {19563812},
title = {{Finding Structure in Time}},
volume = {14},
year = {1990}
}
@article{Lopyrev2015,
abstract = {We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.},
archivePrefix = {arXiv},
arxivId = {1512.01712},
author = {Lopyrev, Konstantin},
eprint = {1512.01712},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Lopyrev - 2015 - Generating News Headlines with Recurrent Neural Networks.pdf:pdf},
journal = {arXiv},
pages = {1--9},
title = {{Generating News Headlines with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.01712},
year = {2015}
}
@article{Lin2004,
abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to auto- matically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of over- lapping units such as n-gram, word sequences, and word pairs between the computer-generated sum- mary to be evaluated and the ideal summaries cre- ated by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summariza- tion evaluation package and their evaluatio ns. Three of them have been used in the Document Under- standing$\backslash$tConference$\backslash$t(DUC)$\backslash$t2004,$\backslash$ta$\backslash$tlarge -scale summarization evaluation sponsored by NIST.},
author = {Lin, C Y},
issn = {00036951},
journal = {Proceedings of the workshop on text summarization branches out (WAS 2004)},
number = {1},
pages = {25--26},
title = {{Rouge: A package for automatic evaluation of summaries}},
year = {2004}
}
@techreport{NationalInstituteofAging2011,
abstract = {Alzheimer's Disease Education and Referral (ADEAR) Center; NIH Publication Number: 02-3782; U.S. Dept of Health and Human Services},
author = {{National Institute of Aging} and {National Institute on Aging}, National institute of Health},
booktitle = {Blood},
doi = {10.1182/blood-2012-09-452755},
institution = {National Institute of Aging},
isbn = {9780160676642},
issn = {1528-0020},
pages = {1--80},
pmid = {23243154},
title = {{Alzheimer's disease: Unraveling the Mystery}},
url = {www.nia.nih.gov/alzheimers},
volume = {120},
year = {2011}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Mikolov2010,
abstract = {In recurrent networks, history is represented by neurons with recurrent connections-history length is unlimited. Also, recurrent networks can learn to compress whole history in low dimensional space, while feedforward networks compress (project) just single word. ...},
author = {Mikolov, T and Karafi{\'{a}}t, M and Burget, L and Cernock{\'{y}}, J},
journal = {Interspeech},
title = {{Recurrent neural network based language model.}},
url = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm{\_}mikolov.pdf{\%}5Cnpapers3://publication/uuid/2FC1A34D-9780-4663-B33B-B64D7EE53AE4},
year = {2010}
}
@article{Rush2015,
abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
archivePrefix = {arXiv},
arxivId = {1509.00685},
author = {Rush, Alexander M and Chopra, Sumit and Weston, Jason},
doi = {10.1162/153244303322533223},
eprint = {1509.00685},
file = {:Users/c.albert/Library/Application Support/Mendeley Desktop/Downloaded/Rush, Chopra, Weston - 2015 - A Neural Attention Model for Abstractive Sentence Summarization.pdf:pdf},
isbn = {9781941643327},
issn = {19909772},
journal = {In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {September},
pages = {379--389},
pmid = {18244602},
title = {{A Neural Attention Model for Abstractive Sentence Summarization}},
url = {http://arxiv.org/abs/1509.00685},
year = {2015}
}
@article{Radev2002a,
abstract = {To provide high-quality and safe care, clinicians must be able to optimally collect, distill, and interpret patient information. Despite advances in text summarization, only limited research exists on clinical summarization, the complex and heterogeneous process of gathering, organizing and presenting patient data in various forms.},
author = {Radev, Dragomir and Hovy, Eduard and McKeown, Kathleen},
doi = {10.1016/j.jbi.2011.03.008},
isbn = {0891-2017},
issn = {1532-0480},
journal = {Computational linguistics},
number = {4},
pages = {399--408},
pmid = {21440086},
title = {{Introduction to the special issue on summarization}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21920798{\%}5Cnhttp://www.mitpressjournals.org/doi/abs/10.1162/089120102762671927},
volume = {28},
year = {2002}
}
@article{Gabielkov2016,
abstract = {This excellent paper has been the subject of some recent social media pranks, the point of which are to show that people rarely read the posts they share on Twitter or Facebook. You can read a Washington Post article about it from mid-June. “People are more willing to share an article than read it,” study co-author Arnaud Legout said in a statement. “This is typical of modern information consumption. People form an opinion based on a summary, or a summary of summaries, without making the effort to go deeper.” The study does go deeper, and in a way that will be of significant interest to analysts; in addition to providing the analysis, it proposes a new metric to measure the influence of a URL. "Ideally," write the authors, "we would like to create a similar metric to quantify the influence of a user," which in the end is suggested via an indirect statistical mechanism.},
author = {Gabielkov, Maksym and Ramachandran, Arthi and Chaintreau, Augustin and Gabielkov, Maksym and Ramachandran, Arthi and Chaintreau, Augustin and Legout, Arnaud and Clicks, Social and Gabielkov, Maksym and Chaintreau, Augustin},
doi = {http://dx.doi.org/10.1145/2896377.2901462},
file = {:Users/c.albert/Research Papers/Dissertation/Misleading Title/6in10share.pdf:pdf},
isbn = {9781450342667},
issn = {0163-5999},
journal = {SIGMETRICS Perform. Eval. Rev.},
keywords = {news media,social clicks,social networks,twitter},
number = {1},
pages = {179--192},
title = {{Social Clicks : What and Who Gets Read on Twitter ? To cite this version : Social Clicks : What and Who Gets Read on Twitter ?}},
url = {https://hal.inria.fr/hal-01281190{\%}5Cnhttp://doi.acm.org/10.1145/2964791.2901462},
volume = {44},
year = {2016}
}
